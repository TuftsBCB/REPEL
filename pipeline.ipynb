{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_sparse(net_file,ngene):\n",
    "    ppi_df = pd.read_csv(net_file,header=None,sep='\\t')\n",
    "    A = np.zeros((ngene,ngene))\n",
    "    row_idx = ppi_df.iloc[:,0].values -1 \n",
    "    col_idx = ppi_df.iloc[:,1].values -1\n",
    "    A[row_idx, col_idx] = ppi_df.iloc[:,2].values\n",
    "    assert (A == A.T).all()\n",
    "    zero_rows = np.all(A == 0, axis=1)\n",
    "    diag_indices = np.arange(ngene)\n",
    "    A[diag_indices[zero_rows], diag_indices[zero_rows]] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_nets(ppi_files,n_gene):\n",
    "    '''\n",
    "    parameters:\n",
    "    - ppi_files: [str, str, ...], list of network file paths, each file should contain three columns: [protein1, protein2, score]\n",
    "    - ref_gene_file: str, file path, the file contains all genes, one gene per line\n",
    "    output:\n",
    "    - nets: n_file x n_gene x n_gene array with ppi networks\n",
    "    '''\n",
    "    n_file = len(ppi_files)\n",
    "    nets = np.zeros((n_file,n_gene,n_gene))\n",
    "    for i in range(n_file):\n",
    "        A = load_network_sparse(ppi_files[i],n_gene)\n",
    "        nets[i,:,:] = A\n",
    "    return nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rwr_original_sparse(ppi_files,restart_prob,ngene,nets):\n",
    "    ''' \n",
    "    - ppi_files: list of network file paths\n",
    "    - restart_prob: RWR restart probability\n",
    "    - ngene: number of genes\n",
    "    output:\n",
    "    - walks: for the i-th RWR result, [i,:,:], each column is the stationary distribution of a node\n",
    "    '''\n",
    "    n_file = len(ppi_files)\n",
    "    e = np.ones(ngene)\n",
    "    I = np.eye(ngene)\n",
    "    walks = np.zeros((n_file,ngene,ngene))\n",
    "    for i in range(n_file):\n",
    "        A = nets[i,:,:]\n",
    "        d = A @ e\n",
    "        P = A / d # transition matrix\n",
    "        W = (I - (1 - restart_prob) * P)\n",
    "        W = np.linalg.inv(W)\n",
    "        W = W * restart_prob \n",
    "        walks[i,:,:] = W\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_embed_sparse_func(walks, ngene, embed_dim):\n",
    "    n_net = walks.shape[0]\n",
    "    mat = np.zeros((ngene,ngene))\n",
    "    W_updated = np.zeros_like(walks)\n",
    "    for i in range(n_net):\n",
    "        W = walks[i,:,:]\n",
    "        W[W<=1e-8] = 0\n",
    "        W = np.log(W, where = W > 1e-8)\n",
    "        W_updated[i,:,:] = W\n",
    "        tmp = W.T @ W\n",
    "        mat = mat + tmp\n",
    "    eigenvalues, eigenvectors = scipy.sparse.linalg.eigs(mat,k=embed_dim)\n",
    "    x = np.diag(np.sqrt(np.sqrt(eigenvalues))) @ eigenvectors.T\n",
    "    return np.real(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_anno(rand,fold,org,ont_type,ont_size1,ont_size2):\n",
    "    '''\n",
    "    predifined fold splits\n",
    "    - rand: 1 2 3 4 5\n",
    "    - fold: 1 2 3 4 5\n",
    "    - org: \"Ecoli\" or \"yeast\"\n",
    "    '''\n",
    "    file_name = 'data/train_test_split/'+org+'/rand' + str(rand) +'/fold' + str(fold) + '_' + ont_type+ '_' +  str(ont_size1)+ '_' +  str(ont_size2)+ '_train_anno.txt'\n",
    "    train = pd.read_csv(file_name,header=None,sep = '\\t')\n",
    "    file_name = 'data/train_test_split/'+org+'/rand' + str(rand) +'/fold' + str(fold) + '_' + ont_type+ '_' +  str(ont_size1)+ '_' +  str(ont_size2)+ '_test_anno.txt'\n",
    "    test = pd.read_csv(file_name,header=None,sep = '\\t')\n",
    "    return train.to_numpy(), test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_graph(nets, ngene, gene_clusters, mustlink_weight, cannotlink_weight):\n",
    "    '''\n",
    "    - nets: original adjacency matrices directly read from PPI files\n",
    "    - gene_clusters: (num_clusers, num_genes), binary matrix indicating which gene belongs to which clusters\n",
    "    '''\n",
    "    n_nets = nets.shape[0]\n",
    "    n_clusters = gene_clusters.shape[0]\n",
    "    augmented = np.zeros((n_nets,(ngene+n_clusters),(ngene+n_clusters)))\n",
    "    for i in range(n_nets):\n",
    "        A = nets[i,:,:]\n",
    "        A_block = np.block([[A,mustlink_weight*gene_clusters.T],[mustlink_weight*gene_clusters,cannotlink_weight*np.ones((n_clusters,n_clusters))]])\n",
    "        np.fill_diagonal(A_block,0)\n",
    "        zero_rows = np.all(np.absolute(A_block) == 0, axis=1)\n",
    "        diag_indices = np.arange(ngene+n_clusters)\n",
    "        A_block[diag_indices[zero_rows], diag_indices[zero_rows]] = 1\n",
    "        augmented[i,:,:] = A_block\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_RWR(augmented_nets, restart_prob):\n",
    "    '''\n",
    "    RWR for augmented graph which contains negative edge weights\n",
    "    '''\n",
    "    n_nets = augmented_nets.shape[0]\n",
    "    n_nodes = augmented_nets.shape[1]\n",
    "    augmented_walks = np.zeros((n_nets,n_nodes,n_nodes))\n",
    "    e = np.ones(n_nodes)\n",
    "    for i in range(n_nets):\n",
    "        A = augmented_nets[i,:,:]\n",
    "        d = np.absolute(A) @ e\n",
    "        L = np.diag(d) - (1-restart_prob)*A\n",
    "        L_inv = np.linalg.inv(L)\n",
    "        W = restart_prob*(np.diag(d) @ L_inv)\n",
    "        augmented_walks[i,:,:] = W\n",
    "    return augmented_walks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_SVD_with_cannolink(aug_walks, embed_dim):\n",
    "    n_net = aug_walks.shape[0]\n",
    "    n_node = aug_walks.shape[1]\n",
    "    mat = np.zeros((n_node,n_node))\n",
    "    W_updated = np.zeros_like(aug_walks)\n",
    "    for i in range(n_net):\n",
    "        W = aug_walks[i,:,:]\n",
    "        min_entry = W.min()\n",
    "        if min_entry > 0:\n",
    "            min_entry = 0.0\n",
    "        W = W - min_entry\n",
    "        W[W<=1e-8] = 0\n",
    "        W = np.log(W, where = W > 1e-8)\n",
    "        W_updated[i,:,:] = W\n",
    "        tmp = W.T @ W\n",
    "        mat = mat + tmp\n",
    "    eigenvalues, eigenvectors = scipy.sparse.linalg.eigs(mat,k=embed_dim)\n",
    "    x = np.diag(np.sqrt(np.sqrt(eigenvalues))) @ eigenvectors.T\n",
    "    return np.real(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_ind(embed,train_anno):\n",
    "    '''\n",
    "    parameters:\n",
    "    - embed: (dim, num_gene), protein embeddings\n",
    "    - train_anno: annotations for training proteins\n",
    "    output:\n",
    "    dist_mat: n_gene x n_gene\n",
    "    sorted_ind: ngene x (ngene-1), top n labels\n",
    "    '''\n",
    "    n_gene = train_anno.shape[1]\n",
    "    train_idx = np.where(sum(train_anno)>0)[0]\n",
    "    # embed = embed[:n_gene,:n_gene]\n",
    "    dist_mat = squareform(pdist(embed.T))\n",
    "    dist_mat = dist_mat[:n_gene,:n_gene] # symmetrical\n",
    "    np.fill_diagonal(dist_mat, 1e8)\n",
    "    # mask = np.ones(n_gene, dtype=bool)\n",
    "    # mask[train_idx] = False\n",
    "    # dist_mat[:, mask] = 1e8\n",
    "\n",
    "    sorted_ind = np.argsort(dist_mat, axis=1)\n",
    "    # sorted_ind = sorted_ind[:,1:] # each row, gene's nearest neighbors\n",
    "    \n",
    "    return dist_mat, sorted_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(dist_mat, knn_mat, train_anno, test_anno,k, weighted=True):\n",
    "    '''\n",
    "    parameters:\n",
    "    - dist_mat: n_gene x n_gene\n",
    "    - knn_mat: ngene x (ngene-1), sorted labels\n",
    "    - train_anno: n_label x n_gene\n",
    "    - test_anno: n_label x n_gene\n",
    "    - k: number of nearest neighbors\n",
    "    - weighted: boolean, whether doing weighted majority vote or not\n",
    "    output:\n",
    "    - final_scores: n_label x n_test, normalized scores of each label\n",
    "    - num_voters: vector of numbers of voting nodes\n",
    "    '''\n",
    "    train_idx = np.where(sum(train_anno)>0)[0]\n",
    "    test_idx = np.where(sum(test_anno)>0)[0]\n",
    "    final_scores = np.zeros((train_anno.shape[0],len(test_idx)))\n",
    "    num_voters = []\n",
    "    updated_voters = []\n",
    "    c = 0\n",
    "    for index, i in enumerate(test_idx):\n",
    "        nn = knn_mat[i,:k]\n",
    "        nn_labeled = nn[np.isin(nn, train_idx)] \n",
    "        \n",
    "        if len(nn_labeled) == 0: # if within the first k neighbors, no neighbor is labeled, then use the nearest neighbor with label\n",
    "            voting_node = knn_mat[i,:][np.isin(knn_mat[i,:], train_idx)][0]\n",
    "            scores = np.array(train_anno[:,voting_node])\n",
    "            scores = scores / sum(scores)\n",
    "            num_voters.append(len(nn_labeled))\n",
    "            tmp = [voting_node]\n",
    "            updated_voters.append(tmp)\n",
    "        else:\n",
    "            votes = np.array(train_anno[:,nn_labeled])\n",
    "            if weighted:\n",
    "                d = dist_mat[i,nn_labeled]\n",
    "                d = d[np.nonzero(d)]\n",
    "                votes = np.array(train_anno[:,nn_labeled[np.nonzero(d)]])\n",
    "                tmp = nn_labeled[np.nonzero(d)]\n",
    "                updated_voters.append(tmp)\n",
    "                num_voters.append(len(d))\n",
    "                if len(d) == 0:\n",
    "                    c += 1\n",
    "                    voting_node = np.random.choice(train_idx)\n",
    "                    scores = np.array(train_anno[:,voting_node])\n",
    "                    scores = scores / sum(scores)\n",
    "                else:\n",
    "                    weights = 1 / d\n",
    "                    scores = votes @ weights.T\n",
    "                    scores = scores / sum(scores)\n",
    "            else:\n",
    "                num_voters.append(len(nn_labeled))\n",
    "                updated_voters.append(nn_labeled)\n",
    "                scores = np.sum(votes,axis=1)\n",
    "                scores = scores / sum(scores)\n",
    "        \n",
    "        final_scores[:,index] = np.squeeze(scores)\n",
    "    print(c)\n",
    "    return final_scores, num_voters,updated_voters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_top1_pred(test_scores, test_anno):\n",
    "    '''\n",
    "    for each test gene, find the label with the highest predicted score, use it as the predicted label\n",
    "    accuracy is defined as (#predicted label in test true labels) / (#test genes)\n",
    "    problems: if there's a tie, the one with smaller index will be used\n",
    "    parameters:\n",
    "    - test_scores: n_label x n_test\n",
    "    - test_anno: n_label x n_gene\n",
    "    output:\n",
    "    - acc: accuracy score\n",
    "    '''\n",
    "    test_idx = np.where(sum(test_anno)>0)[0]\n",
    "    zero_idx = np.where(np.sum(test_scores,axis=0)==0)[0]\n",
    "    mask = np.ones(len(test_idx), dtype=bool)\n",
    "    mask[zero_idx] = False\n",
    "    test_anno = test_anno[:,test_idx] # n_label x n_test\n",
    "    sorted_index = np.argsort(-1*test_scores,axis=0) # n_label x n_test, with row 0 the highest predicted label for each gene\n",
    "    true_pred = test_anno[sorted_index[0,:], np.arange(test_anno.shape[1])]\n",
    "    true_pred = true_pred[mask]\n",
    "    print(len(zero_idx))\n",
    "    acc = np.mean(true_pred)\n",
    "    return acc,true_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_auprc_pred(test_scores, test_anno,top_n):\n",
    "    '''\n",
    "    for each test gene, find the labels with the top_n highest predicted scores, use them as the predicted labels\n",
    "    f1 is defined as 2*TP / 2*TP + FP + FN\n",
    "    probelms: if there's a tie, the one with smaller index will be used, only top n predictions will be considered, it will increase the number of FN\n",
    "    parameters:\n",
    "    - test_scores: n_label x n_test\n",
    "    - test_anno: n_label x n_gene\n",
    "    - top_n: int, the number of labels to be predicted\n",
    "    output:\n",
    "    - acc: accuracy score\n",
    "    '''\n",
    "    test_idx = np.where(sum(test_anno)>0)[0]\n",
    "    zero_idx = np.where(np.sum(test_scores,axis=0)==0)[0]\n",
    "\n",
    "    mask = np.ones(len(test_idx), dtype=bool)\n",
    "    mask[zero_idx] = False\n",
    "    test_anno = test_anno[:,test_idx] # n_label x n_test\n",
    "    \n",
    "    test_anno = test_anno[:,mask]\n",
    "    test_scores = test_scores[:,mask]\n",
    "    sorted_index = np.argsort(-1*test_scores,axis=0) # n_label x n_test, with row 0 the highest predicted label for each gene\n",
    "    top_ind = sorted_index[:top_n,:].flatten()\n",
    "    pred = np.zeros_like(test_anno)\n",
    "    cols = np.tile(np.arange(test_anno.shape[1]), top_n)\n",
    "    pred[top_ind, cols] = 1\n",
    "    f1 = sklearn.metrics.f1_score(test_anno.flatten(),pred.flatten())\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(test_anno.flatten(), pred.flatten())\n",
    "    auprc = sklearn.metrics.auc(recall, precision)\n",
    "    return f1, auprc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for each augmented node, randomly choose a fixed number of nodes to connect to\n",
    "the number of genes that each augmented node connects to are the same except for the last one\n",
    "'''\n",
    "def random_split_vector(train_anno,n_gene, num_sub_vectors,seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    input_vector = np.where(sum(train_anno)>0)[0]\n",
    "    \n",
    "    if num_sub_vectors <= 0 or num_sub_vectors > len(input_vector):\n",
    "        raise ValueError(\"Invalid number of sub-vectors\")\n",
    "    \n",
    "    shuffled_vector = np.random.permutation(input_vector)\n",
    "    sub_vector_size = len(shuffled_vector) // num_sub_vectors\n",
    "    \n",
    "    group_matrix = np.zeros((num_sub_vectors, len(input_vector)), dtype=int)\n",
    "    res_matrix = np.zeros((num_sub_vectors, n_gene), dtype=int)\n",
    "    \n",
    "    start_index = 0\n",
    "    for i in range(num_sub_vectors):\n",
    "        end_index = start_index + sub_vector_size\n",
    "        \n",
    "        if i == num_sub_vectors - 1:\n",
    "            end_index = len(shuffled_vector)\n",
    "        \n",
    "        selected_indices = shuffled_vector[start_index:end_index]\n",
    "        \n",
    "        group_matrix[i, np.isin(shuffled_vector, selected_indices)] = 1\n",
    "        # res_matrix[i, selected_indices] = 1\n",
    "        \n",
    "        start_index = end_index\n",
    "    \n",
    "    res_matrix[:,shuffled_vector] = group_matrix\n",
    "    \n",
    "    return res_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(ppi_files,n_gene,method=None,restart_prob=None,embed_dim=None,rand=None,org=None,n_fold=None,k=None,ont_type=None,ont_size1=None,ont_size2=None,n_cluster=None):\n",
    "    ''' \n",
    "    parameters:\n",
    "    - ppi_files: list of str, list of file paths to ppi networks\n",
    "    - n_gene: int, number of genes\n",
    "    - method: list of str, one or more of Mashup, REPEL\n",
    "    - restart_prob: float, RWR restart probability\n",
    "    - embed_dim: int, number of dimension\n",
    "    - rand: int, random split\n",
    "    - org: str, \"yeast\" or \"Ecoli\" \n",
    "    - n_fold: int, total number of folds\n",
    "    - k: int, number of nearest neighbors to be considered\n",
    "    - ont_type: str, bp or mf or cc\n",
    "    - ont_size1: int, 11, 31, 101\n",
    "    - ont_size2: int, 30, 100, 300\n",
    "    - n_cluster: int, number of random augmented nodes\n",
    "    output:\n",
    "    - performance_dict: a dictionary contains list of performances for all methods\n",
    "    '''\n",
    "\n",
    "    performance_dict = {}\n",
    "\n",
    "    for m in method:\n",
    "        m_acc = m + \"_acc\"\n",
    "        m_f1 = m + \"_f1\"\n",
    "        m_auprc = m + \"_auprc\"\n",
    "        performance_dict[m_acc] = []\n",
    "        performance_dict[m_f1] = []\n",
    "        performance_dict[m_auprc] = []\n",
    "        for i in range(n_fold):\n",
    "            print(\"fold: \", i+1)\n",
    "            train_anno, test_anno = load_train_test_anno(rand,i+1,org,ont_type,ont_size1,ont_size2)\n",
    "            if m == \"Mashup\":\n",
    "                print(\"Mashup\")\n",
    "                nets = load_all_nets(ppi_files,n_gene)\n",
    "                walks = compute_rwr_original_sparse(ppi_files,restart_prob,n_gene,nets)\n",
    "                x = svd_embed_sparse_func(walks, n_gene, embed_dim)\n",
    "                dist_mat, knn = get_knn_ind(x,train_anno)\n",
    "                scores, _, _ = majority_vote(dist_mat, knn, train_anno, test_anno,k)\n",
    "                acc,_ = acc_top1_pred(scores, test_anno)\n",
    "                f1, auprc = f1_auprc_pred(scores, test_anno,3)\n",
    "                performance_dict[m_acc].append(acc)\n",
    "                performance_dict[m_f1].append(f1)\n",
    "                performance_dict[m_auprc].append(auprc)\n",
    "            elif m == \"REPEL\":\n",
    "                print(\"REPEL\")\n",
    "                nets = load_all_nets(ppi_files,n_gene)\n",
    "                rand_cluster = random_split_vector(train_anno,n_gene, n_cluster,seed=None)\n",
    "                rand_graph = augment_graph(nets, n_gene, rand_cluster, 1, -1)\n",
    "                rand_rwr_res = augmented_RWR(rand_graph, restart_prob)\n",
    "                mat_rand_x = augmented_SVD_with_cannolink(rand_rwr_res, embed_dim)\n",
    "                dist_mat, knn = get_knn_ind(mat_rand_x,train_anno)\n",
    "                scores, _,_ = majority_vote(dist_mat, knn, train_anno, test_anno,k, weighted=True)\n",
    "                acc,_ = acc_top1_pred(scores, test_anno)\n",
    "                f1, auprc = f1_auprc_pred(scores, test_anno,3)\n",
    "                performance_dict[m_acc].append(acc)\n",
    "                performance_dict[m_f1].append(f1)\n",
    "                performance_dict[m_auprc].append(auprc)\n",
    "\n",
    "            else:\n",
    "                print(\"Haven't implemented yet\")\n",
    "                return\n",
    "    return performance_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(performance_dict,rand,org,ont_type,ont_size1,ont_size2,save_path=None):\n",
    "    with open(save_path,\"a\") as f:\n",
    "        tmp = org + \" \" + \"rand \" + str(rand) + \" \" + ont_type + \" \" + str(ont_size1) + \" \" + str(ont_size2) \n",
    "        f.write(tmp)\n",
    "        f.write(\"\\n\")\n",
    "        for k, v in performance_dict.items():\n",
    "            f.write(k)\n",
    "            f.write(\" \")\n",
    "            for i in v:\n",
    "                f.write(f\"{i:.4f}\")\n",
    "                f.write(\" \")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for yeast\n",
    "string_nets = ['neighborhood', 'fusion', 'cooccurence', 'coexpression', 'experimental', 'database']\n",
    "ppi_files = []\n",
    "for net in string_nets:\n",
    "    tmp = 'data/networks/yeast/yeast_string_'+net+'_adjacency.txt'\n",
    "    ppi_files.append(tmp)\n",
    "\n",
    "all_genes = pd.read_csv(\"data/annotations/yeast/go_yeast_ref_genes.txt\",header=None)\n",
    "all_genes = list(all_genes.iloc[:,0].values)\n",
    "n_gene = len(all_genes)\n",
    "print(ppi_files)\n",
    "print(n_gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method=['REPEL','Mashup']\n",
    "restart_prob=0.5\n",
    "embed_dim=400\n",
    "org=\"yeast\"\n",
    "n_fold=5\n",
    "k=10\n",
    "ont_type_list=[\"bp\",\"mf\",\"cc\"]\n",
    "ont_size1_list=[11,31,101]\n",
    "ont_size2_list=[30,100,300]\n",
    "n_cluster=15\n",
    "for rand in range(1,6):\n",
    "    print(\"rand: \",rand)\n",
    "    for ont_type in ont_type_list:\n",
    "        print(\"ont_type\")\n",
    "        for c in range(3):\n",
    "            ont_size1 = ont_size1_list[c]\n",
    "            ont_size2 = ont_size2_list[c]\n",
    "            performance_dict = run_pipeline(ppi_files,n_gene,method=method,restart_prob=restart_prob,embed_dim=embed_dim,rand=rand,org=org,n_fold=n_fold,k=k,ont_type=ont_type,ont_size1=ont_size1,ont_size2=ont_size2,n_cluster=n_cluster)\n",
    "            write_log(performance_dict,rand,org,ont_type,ont_size1,ont_size2,save_path=\"yeast_result_log.txt\")\n",
    "            break\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For E. coli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method=['MashUp','REPEL']\n",
    "restart_prob=0.5\n",
    "embed_dim=400\n",
    "org=\"yeast\"\n",
    "n_fold=5\n",
    "k=10\n",
    "ont_type_list=[\"bp\",\"mf\",\"cc\"]\n",
    "ont_size1_list=[11,31,101]\n",
    "ont_size2_list=[30,100,300]\n",
    "n_cluster=20\n",
    "for rand in range(1,6):\n",
    "    print(\"rand: \",rand)\n",
    "    for ont_type in ont_type_list:\n",
    "        print(\"ont_type\")\n",
    "        for c in range(3):\n",
    "            ont_size1 = ont_size1_list[c]\n",
    "            ont_size2 = ont_size2_list[c]\n",
    "            performance_dict = run_pipeline(ppi_files,n_gene,method=method,restart_prob=restart_prob,embed_dim=embed_dim,rand=rand,org=org,n_fold=n_fold,k=k,ont_type=ont_type,ont_size1=ont_size1,ont_size2=ont_size2,n_cluster=n_cluster)\n",
    "            write_log(performance_dict,rand,org,ont_type,ont_size1,ont_size2,save_path=\"yeast_result_log.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for E.coli\n",
    "string_nets = ['neighborhood', 'fusion', 'cooccurence', 'coexpression', 'experimental', 'database']\n",
    "ppi_files = []\n",
    "for net in string_nets:\n",
    "    tmp = 'data/networks/Ecoli/ecoli_string_'+net+'_adjacency.txt'\n",
    "    ppi_files.append(tmp)\n",
    "\n",
    "all_genes = pd.read_csv(\"data/annotations/Ecoli/ecoli_ref_genes.txt\",header=None,sep='\\t')\n",
    "all_genes = list(all_genes.iloc[:,1].values)\n",
    "n_gene = len(all_genes)\n",
    "print(ppi_files)\n",
    "print(n_gene)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method=['Mashup','REPEL']\n",
    "restart_prob=0.5\n",
    "embed_dim=400\n",
    "org=\"Ecoli\"\n",
    "n_fold=5\n",
    "k=10\n",
    "ont_type_list=[\"bp\",\"mf\",\"cc\"]\n",
    "ont_size1_list=[11,31,101]\n",
    "ont_size2_list=[30,100,300]\n",
    "n_cluster=10\n",
    "for rand in range(1,6):\n",
    "    print(\"rand: \",rand)\n",
    "    for ont_type in ont_type_list:\n",
    "        print(\"ont_type\")\n",
    "        for c in range(3):\n",
    "            ont_size1 = ont_size1_list[c]\n",
    "            ont_size2 = ont_size2_list[c]\n",
    "            performance_dict = run_pipeline(ppi_files,n_gene,method=method,restart_prob=restart_prob,embed_dim=embed_dim,rand=rand,org=org,n_fold=n_fold,k=k,ont_type=ont_type,ont_size1=ont_size1,ont_size2=ont_size2,n_cluster=n_cluster)\n",
    "            write_log(performance_dict,rand,org,ont_type,ont_size1,ont_size2,save_path=\"Ecoli_result_log_rerun.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bionic_embedding_yeast = pd.read_csv(\"bionic/outputs/yeast_mu_features.tsv\",sep=\"\\t\",header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bionic_embedding_yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bionic_embedding_yeast.loc[3].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = load_all_nets(ppi_files,n_gene)\n",
    "walks = compute_rwr_original_sparse(ppi_files,0.5,n_gene,nets)\n",
    "mu_x = svd_embed_sparse_func(walks, n_gene, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_w_bionic(bionic_embed_df,mu_x,rand,org,ont_type,ont_size1,ont_size2,k):\n",
    "    bionix_id = np.array(bionic_embed_df.index-1)\n",
    "    merge_embed = mu_x.T\n",
    "    perf = {}\n",
    "    perf[\"bionic_acc\"] = []\n",
    "    perf[\"bionic_f1\"] = []\n",
    "    perf[\"bionic_auprc\"] = []\n",
    "    for ele in bionix_id:\n",
    "        merge_embed[ele,:] = bionic_embed_df.loc[ele+1].to_numpy()\n",
    "\n",
    "    for i in range(5):\n",
    "        train_anno, test_anno = load_train_test_anno(rand,i+1,org,ont_type,ont_size1,ont_size2)\n",
    "        dist_mat, knn = get_knn_ind(merge_embed.T,train_anno)\n",
    "        scores, _, _ = majority_vote(dist_mat, knn, train_anno, test_anno,k)\n",
    "        acc,_ = acc_top1_pred(scores, test_anno)\n",
    "        f1, auprc = f1_auprc_pred(scores, test_anno,3)\n",
    "        perf[\"bionic_acc\"].append(acc)\n",
    "        perf[\"bionic_f1\"].append(f1)\n",
    "        perf[\"bionic_auprc\"].append(auprc)\n",
    "    return perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org=\"yeast\"\n",
    "k=10\n",
    "ont_type_list=[\"bp\",\"mf\",\"cc\"]\n",
    "ont_size1_list=[11,31,101]\n",
    "ont_size2_list=[30,100,300]\n",
    "for rand in range(1,6):\n",
    "    print(\"rand: \",rand)\n",
    "    for ont_type in ont_type_list:\n",
    "        print(\"ont_type\")\n",
    "        for c in range(3):\n",
    "            ont_size1 = ont_size1_list[c]\n",
    "            ont_size2 = ont_size2_list[c]\n",
    "            perf = compare_w_bionic(bionic_embedding_yeast,mu_x,rand,org,ont_type,ont_size1,ont_size2,k)\n",
    "            write_log(perf,rand,org,ont_type,ont_size1,ont_size2,save_path=\"Bionic_yeast.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = load_all_nets(ppi_files,n_gene)\n",
    "walks = compute_rwr_original_sparse(ppi_files,0.5,n_gene,nets)\n",
    "mu_x = svd_embed_sparse_func(walks, n_gene, 512)\n",
    "mu_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bionic_embedding_ecoli = pd.read_csv(\"bionic/outputs/ecoli_mu_features.tsv\",sep=\"\\t\",header=0,index_col=0)\n",
    "bionic_embedding_ecoli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org=\"Ecoli\"\n",
    "k=10\n",
    "ont_type_list=[\"bp\",\"mf\",\"cc\"]\n",
    "ont_size1_list=[11,31,101]\n",
    "ont_size2_list=[30,100,300]\n",
    "for rand in range(1,6):\n",
    "    print(\"rand: \",rand)\n",
    "    for ont_type in ont_type_list:\n",
    "        print(\"ont_type\")\n",
    "        for c in range(3):\n",
    "            ont_size1 = ont_size1_list[c]\n",
    "            ont_size2 = ont_size2_list[c]\n",
    "            perf = compare_w_bionic(bionic_embedding_ecoli,mu_x,rand,org,ont_type,ont_size1,ont_size2,k)\n",
    "            write_log(perf,rand,org,ont_type,ont_size1,ont_size2,save_path=\"Bionic_ecoli_rerun.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepNF_features = np.load('deepNF/ori_yeast_midmodel_features_dim600.npy') \n",
    "deepNF_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anno, test_anno = load_train_test_anno(1,1,\"yeast\",\"bp\",11,30)\n",
    "dist_mat, knn = get_knn_ind(deepNF_features.T,train_anno)\n",
    "scores, _, _ = majority_vote(dist_mat, knn, train_anno, test_anno,10)\n",
    "acc,_ = acc_top1_pred(scores, test_anno)\n",
    "f1, auprc = f1_auprc_pred(scores, test_anno,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_w_deepNF(deepNF_embed,rand,org,ont_type,ont_size1,ont_size2,k):\n",
    "    perf = {}\n",
    "    perf[\"deepNF_acc\"] = []\n",
    "    perf[\"deepNF_f1\"] = []\n",
    "    perf[\"deepNF_auprc\"] = []\n",
    "    for i in range(5):\n",
    "        train_anno, test_anno = load_train_test_anno(rand,i+1,org,ont_type,ont_size1,ont_size2)\n",
    "        dist_mat, knn = get_knn_ind(deepNF_embed.T,train_anno)\n",
    "        scores, _, _ = majority_vote(dist_mat, knn, train_anno, test_anno,k)\n",
    "        acc,_ = acc_top1_pred(scores, test_anno)\n",
    "        f1, auprc = f1_auprc_pred(scores, test_anno,3)\n",
    "        perf[\"deepNF_acc\"].append(acc)\n",
    "        perf[\"deepNF_f1\"].append(f1)\n",
    "        perf[\"deepNF_auprc\"].append(auprc)\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org=\"yeast\"\n",
    "k=10\n",
    "ont_type_list=[\"bp\",\"mf\",\"cc\"]\n",
    "ont_size1_list=[11,31,101]\n",
    "ont_size2_list=[30,100,300]\n",
    "for rand in range(1,6):\n",
    "    print(\"rand: \",rand)\n",
    "    for ont_type in ont_type_list:\n",
    "        print(\"ont_type:\", ont_type)\n",
    "        for c in range(3):\n",
    "            ont_size1 = ont_size1_list[c]\n",
    "            ont_size2 = ont_size2_list[c]\n",
    "            perf = compare_w_deepNF(deepNF_features,rand,org,ont_type,ont_size1,ont_size2,k)\n",
    "            write_log(perf,rand,org,ont_type,ont_size1,ont_size2,save_path=\"DeepNF_yeast.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepNF_features_ecoli = np.load('deepNF/ecoli_midmodel_features_dim600.npy') \n",
    "deepNF_features_ecoli.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org=\"Ecoli\"\n",
    "k=10\n",
    "ont_type_list=[\"bp\",\"mf\",\"cc\"]\n",
    "ont_size1_list=[11,31,101]\n",
    "ont_size2_list=[30,100,300]\n",
    "for rand in range(1,6):\n",
    "    print(\"rand: \",rand)\n",
    "    for ont_type in ont_type_list:\n",
    "        print(\"ont_type\")\n",
    "        for c in range(3):\n",
    "            ont_size1 = ont_size1_list[c]\n",
    "            ont_size2 = ont_size2_list[c]\n",
    "            perf = compare_w_deepNF(deepNF_features_ecoli,rand,org,ont_type,ont_size1,ont_size2,k)\n",
    "            write_log(perf,rand,org,ont_type,ont_size1,ont_size2,save_path=\"DeepNF_ecoli_rerun.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anno, test_anno = load_train_test_anno(1,1,\"Ecoli\",\"cc\",11,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
